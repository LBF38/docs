"use strict";(self.webpackChunk_logto_docs=self.webpackChunk_logto_docs||[]).push([[5299],{87407:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"logto-x-hasura","metadata":{"permalink":"/zh-cn/blog/logto-x-hasura","source":"@site/blog/2022-08-20-logto-x-hasura/index.mdx","title":"Logto x Hasura: How to use open-source auth + GraphQL solution to boost your project","description":"Intro","date":"2022-08-20T00:00:00.000Z","formattedDate":"2022\u5e748\u670820\u65e5","tags":[{"label":"graphql","permalink":"/zh-cn/blog/tags/graphql"},{"label":"hasura","permalink":"/zh-cn/blog/tags/hasura"},{"label":"api","permalink":"/zh-cn/blog/tags/api"}],"readingTime":5.165,"hasTruncateMarker":true,"authors":[{"name":"Gao","title":"Founder of Silverhand","url":"https://github.com/gao-sun","imageURL":"https://github.com/gao-sun.png","key":"gao"}],"frontMatter":{"slug":"logto-x-hasura","authors":"gao","tags":["graphql","hasura","api"]},"nextItem":{"title":"TypeScript all-in-one: Monorepo with its pains and gains","permalink":"/zh-cn/blog/typescript-all-in-one"}},"content":"import logos from \'./logos.png\';\\nimport createApi from \'./create-api.png\';\\n\\n\\n## Intro\\n\\nWhen it comes to a new project, you usually cannot skip several things: APIs, authentication + authorization, identity, and end-user sign-in flow. It used to be hard to kick off these things because there are many concepts and technologies that spread widely: RESTful/GraphQL, web frontend, native client, connect clients with APIs, auth best practices to balance security and user experience, etc.\\n\\nAlso, most of the works are \u201crepeating\u201d. I mean, they are needed and similar for almost every project, with some tweaks.\\n\\n\x3c!--truncate--\x3e\\n\\nSounds scary and tedious? Don\u2019t panic. Today we have open source. With the two open-source projects below, things become not tricky :\\n\\n- [Logto](https://github.com/logto-io/logto): Helps you build the sign-in, auth, and user identity within minutes.\\n- [Hasura](https://github.com/hasura/graphql-engine): Blazing fast, instant real-time GraphQL APIs on your DB with fine-grained access control.\\n\\n<center><img alt=\\"Logto x Hasura\\" src={logos} width=\\"480\\" /></center>\\n\\nIn this article, we\u2019ll focus on connecting Logto and Hasura, which enables you to implement authentication, authorization, and GraphQL APIs without friction. Thus you can quickly jump into your business without rocket-science learning.\\n\\n\\n## Get started\\n\\n### Prerequisites\\n\\nSince both [Logto](https://docs.logto.io/docs/tutorials/get-started/) and [Hasura](https://hasura.io/docs/latest/getting-started/index/) have a decent get-started guide, we assume you have read them and have a basic feeling. Access to a running instance of both is needed.\\n\\nWe assume the accessible endpoints are:\\n\\n- Logto: `http://localhost:3001`\\n- Hasura: `http://localhost:8080`\\n\\n:::info\\nIf you are using Docker/Docker Compose, for accessing of your machine(host)\u2019s `localhost`, you can use the Docker magic string `host.docker.internal`. In this case, the Logto endpoint will be `http://host.docker.internal:3001`.\\n:::\\n\\nAlso, we assume you have a preferred platform and framework to build the client app, say React or Next.js.\\n\\n### Set up API in Logto\\n\\nIn the left navigation sidebar of your Logto Admin Console, click \u201cAPI Resources\u201d, and you\u2019ll see the API Resource management page.\\n\\nThen click the huge \u201c+ Create API Resource\u201d button in the top-right corner. In the opening modal, enter `Hasura` for API name and `https://hasura.api` for API identifier.\\n\\n<center><img alt=\\"Create API modal\\" src={createApi} width=\\"540\\" /></center>\\n\\nWe\u2019ll use this API identifier for the rest of our article. But feel free to change the values based on your preference.\\n\\nClick \u201cCreate API Resource\u201d, and it will show a message that indicates the resource has been successfully created. That\u2019s all we need in Logto for now.\\n\\n### Enable webhook authentication in Hasura\\n\\n[Hasura uses role-based access management](https://hasura.io/docs/latest/auth/index/), which handles authorization. So, we only need to figure out authentication. It supports two methods: Webhook and JWT. We choose [webhook](https://hasura.io/docs/latest/auth/authentication/webhook/) since it\u2019s more flexible.\\n\\nTo enable webhook authentication, you must set the admin secret and auth hook endpoint.\\n\\n- The admin secret is the key to having Hasura admin access when sending requests. It is required before enabling webhook authentication. Remember to keep it somewhere safe, and don\u2019t use it in production.\\n- The auth hook endpoint is a URL to send authentication requests.\\n\\nYou can set them via [environment variables](https://hasura.io/docs/latest/auth/authentication/webhook/#configuring-webhook-mode):\\n\\n```yaml\\nHASURA_GRAPHQL_ADMIN_SECRET: myadminsecretkey # Replace with your own secret\\nHASURA_GRAPHQL_AUTH_HOOK: http://localhost:3001/api/authn/hasura?resource=https://hasura.api\\n```\\n\\nYou may notice we use the API identifier filled in Logto to build the auth hook endpoint. It ensures that the user is passing the correct bearer token instead of a random one that may from malicious.\\n\\nYou need to update the auth hook endpoint if you have a different Logto endpoint or API indicator. Say you have `https://logto.domain.com` as the Logto endpoint and `https://graphql.domain.com` as the API identifier, then it will be:\\n\\n```yaml\\nHASURA_GRAPHQL_AUTH_HOOK: https://logto.domain.com/api/authn/hasura?resource=https://graphql.domain.com\\n```\\n\\nFrom now, for every GraphQL request, Hasura will bring all request headers to the Logto auth hook endpoint, and Logto will respond properly.\\n\\n## Send secured GraphQL requests\\n\\n### Summary\\n\\nSince we won\u2019t use the Hasura admin secret in production, every GraphQL request is secured by the following headers:\\n\\n- `Authorization` The standard bearer token that Logto generates.\\n- `Expected-Role` The role you want Logto to show in the auth hook response.\\n\\n:::caution\\nIf the user that the `Authorization` header indicates doesn\u2019t have the `Expected-Role`, Logto will respond with `401 Unauthorized`.\\n:::\\n\\nThe `Authorization` header requires a valid Access Token in JWT format with the Hasura API indicator for audience. Hold on - it\u2019s quite hard to remember and compose all these things. Fortunately we get Logto SDKs to simplify the geeky part.\\n\\n### Set default roles in Logto\\n\\nBy default, only the first user will have an `admin` role name. After that, Logto will NOT assign any role names to new users. But for Hasura, it is necessary to have a role to perform an authed request.\\n\\nWhile access control is still an under-the-hood feature of Logto, we don\u2019t want you to manually add the default role names. You can set an environment variable `USER_DEFAULT_ROLE_NAMES` with a comma-separated string [for Logto](https://docs.logto.io/docs/references/core/configuration). E.g.:\\n\\n```yaml\\nUSER_DEFAULT_ROLE_NAMES: user,good_user\\n```\\n\\nThen two roles `user` and `good_user` will be automatically added to newly created users. It will reflect in both `users` table and Access Tokens.\\n\\n### Integrate Logto SDK\\n\\nFollow the [integration guide](https://docs.logto.io/docs/recipes/integrate-logto/) to integrate a Logto SDK in your client app. It enables not only the ability to generate a valid Access Token for GraphQL requests, but also a smooth sign-in experience for your end-users.\\n\\nOnce you finish the guide, we need one tiny modification to the `LogtoConfig`: Add the API indicator you created in Logto Admin Console to `resources`. Taking React SDK as an example:\\n\\n```tsx\\nconst config: LogtoConfig = {\\n  endpoint: \'http://localhost:3001\',\\n  appId: \'<your-application-id>\',\\n  resources: [\'https://hasura.api\'], // Add this line\\n};\\n```\\n\\n### Send requests\\n\\nFinally! After the user is signed in, use `getAccessToken()` in Logto SDK to fetch the Access Token for Hasura GraphQL requests:\\n\\n```tsx\\nconst accessToken = await logto.getAccessToken(\'https://hasura.api\');\\n\\n// Before sending the request\\nrequest.headers.set(\'Authorization\', `Bearer ${accessToken}`);\\nrequest.headers.set(\'Expected-Role\', \'user\');\\n```\\n\\n## Recap\\n\\nWith the effort above, we successfully implemented all the non-skippable things in the intro:\\n\\n- A database-schema-driven GraphQL API endpoint\\n- An auth and identity service on top of OIDC protocol\\n- The complete end-user sign-in flow and auth state management\\n- Secured API access based on user identity and roles\\n\\nNot that hard, right? If you meet any issues, feel free to join the [Logto](https://discord.gg/vRvwuwgpVX) or [Hasura](https://discord.gg/hasura) discord server to have a live chat with the team."},{"id":"typescript-all-in-one","metadata":{"permalink":"/zh-cn/blog/typescript-all-in-one","source":"@site/blog/2022-08-07-typescript-all-in-one.md","title":"TypeScript all-in-one: Monorepo with its pains and gains","description":"Intro","date":"2022-08-07T00:00:00.000Z","formattedDate":"2022\u5e748\u67087\u65e5","tags":[{"label":"typescript","permalink":"/zh-cn/blog/tags/typescript"},{"label":"monorepo","permalink":"/zh-cn/blog/tags/monorepo"}],"readingTime":8.93,"hasTruncateMarker":true,"authors":[{"name":"Gao","title":"Founder of Silverhand","url":"https://github.com/gao-sun","imageURL":"https://github.com/gao-sun.png","key":"gao"}],"frontMatter":{"slug":"typescript-all-in-one","authors":"gao","tags":["typescript","monorepo"]},"prevItem":{"title":"Logto x Hasura: How to use open-source auth + GraphQL solution to boost your project","permalink":"/zh-cn/blog/logto-x-hasura"}},"content":"## Intro\\n\\nI always had a dream of monorepo.\\n\\nI saw the monorepo approach while working for Airbnb, but it was for the frontend only. With a deep love of the JavaScript ecosystem and the \u201chappy\u201d TypeScript developing experience, I started to align frontend and backend code in the same language from ~three years ago. It was great (for hiring) but not that great for developing since our projects were still scattered across multiple repos.\\n\\n\x3c!--truncate--\x3e\\n\\n:::info FYI\\nThere are quotes around the word \u201chappy\u201d since TypeScript did bring me a lot of fun and a-ha moments, but it also let me think \u201chow could this doesn\u2019t work\u201d sometimes.\\n:::\\n\\nAs it says, \u201cthe best way of refactoring a project is to start a new one\u201d. So when I was starting my startup about one year ago, I decided to use a total monorepo strategy: put frontend and backend projects, even database schemas, into one repo.\\n\\nIn this article, I won\u2019t compare monorepo and polyrepo since it\u2019s all about philosophy. Instead, I\u2019ll focus on the building and evolving experience and assume you are familiar with the JS/TS ecosystem.\\n\\nThe final result is available on [GitHub](https://github.com/logto-io/logto).\\n\\n## Why TypeScript?\\n\\nFrankly speaking, I\u2019m a fan of JavaScript and TypeScript. I love the compatibility of its flexibility and rigorousness: you can fall back to `unknown` or `any` (although we banned any form of `any` in our codebase), or use a super-strict lint rule set to align the code style across the team.\\n\\nWhen we were talking about the concept of \u201cfullstack\u201d before, we usually imagine at least two ecosystems and programming languages: one for frontend and one for backend.\\n\\nOne day, I suddenly realized it could be simpler: Node.js is fast enough (believe me, in most cases, code quality is more important than running speed), TypeScript is mature enough (works well in big frontend projects), and the monorepo concept has been practiced by a bunch of famous teams (React, Babel, etc.) - so why don\u2019t we combine all the code together, from frontend to backend? This can make engineers do the jobs without context switch in one repo and implement a complete feature in (almost) one language.\\n\\n## Choosing package manager\\n\\nAs a developer, and as usual, I couldn\u2019t wait to start coding. But this time, things were different.\\n\\nThe choice of the package manager is critical to the dev experience in a monorepo.\\n\\n:::info \ud83d\udd28 TL; DR\\nWe chose lerna with pnpm.\\n:::\\n\\n### The pain of inertia\\n\\nIt was July 2021. I started with `yarn@1.x` since I\u2019ve been using it for a long time. Yarn was fast, but soon I met several issues with Yarn Workspaces. E.g., [not hoisting dependencies correctly](https://github.com/yarnpkg/yarn/issues/7572), and tons of issues are tagged with \u201c[fixed in modern](https://github.com/yarnpkg/yarn/issues?q=label%3Afixed-in-modern+)\u201d, which redirects me to the v2 ([berry](https://github.com/yarnpkg/berry)).\\n\\n\u201cOkay fine I\u2019m upgrading now.\u201d I stopped struggling with v1 and started to migrate. But the long [migration guide](https://yarnpkg.com/getting-started/migration) of berry frightened me, and I gave up after several failed tries.\\n\\n### It just works\\n\\nSo the research about package managers began. I was absorbed by `pnpm` after a trial: fast as yarn, native monorepo support, similar commands to `npm`, hard links, etc. Most importantly, it just works. As a developer who wants to get started with a product but NOT develop a package manager, I just wanted to add some dependencies and start the project without knowing how a package manager works or any other fancy concepts.\\n\\nBased on the same idea, we chose an old friend `lerna` for executing commands across the packages and publishing workspace packages.\\n\\n:::info\\nNow pnpm has a -w option to run commands in the workspace root and --filter for filtering. Thus you can probably replace lerna with a more dedicated package publishing CLI.\\n:::\\n\\n## Defining package scopes\\n\\nIt\u2019s hard to clearly figure out the final scope of each package in the beginning. Just start with your best try according to the status quo, and remember you can always refactor during development.\\n\\nOur [initial structure](https://github.com/logto-io/logto/tree/af7e6ccd83723d623555dafa4650e115fa795838/packages) contains four packages:\\n\\n- `core`: the backend monolith service.\\n- `phrases`: i18n key \u2192 phrase resources.\\n- `schemas`: the database and shared TypeScript schemas.\\n- `ui`: a web SPA that interacts with `core`.\\n\\n## Tech stack for fullstack\\n\\nSince we are embracing the JavaScript ecosystem and using TypeScript as our main programming language, a lot of choices are straightforward (based on my preference \ud83d\ude0a):\\n\\n- `koajs` for the backend service (core): I had a hard experience using `async/await` in `express`, so I decided to use something with native support.\\n- `i18next/react-i18next` for i18n (phrases/ui): like its simplicity of APIs and good TypeScript support.\\n- `react` for SPA (ui): Just personal preference.\\n\\n### How about schemas?\\n\\nSomething is still missing here: database system and schema <-> TypeScript definition mapping.\\n\\n#### General v.s. opinionated\\n\\nAt that point, I tried two popular approaches:\\n\\n- Use ORM with a lot of decorators.\\n- Use a query builder like Knex.js.\\n\\nBut both of them produce a strange feeling during previous development:\\n\\n- For ORM: I\u2019m not a fan of decorators, and another abstract layer of the database causes more learning effort and uncertainty for the team.\\n- For query builder: It\u2019s like writing SQL with some restrictions (in a good way), but it\u2019s not actual SQL. Thus we need to use `.raw()` for raw queries in many scenarios.\\n\\nThen I saw this article: \u201c[Stop using Knex.js: Using SQL query builder is an anti-pattern](https://gajus.medium.com/stop-using-knex-js-and-earn-30-bf410349856c)\u201d. The title looks aggressive, but the content is great. It strongly reminds me that \u201cSQL is a programming language\u201d, and I realized I could write SQL directly (just like CSS, how could I miss this!) to leverage the native language and database features instead of adding another layer and reducing the power.\\n\\nIn conclusion, I decided to stick with Postgres and [Slonik](https://github.com/gajus/slonik) (an open-source Postgres client), as the article states:\\n\\n> \u2026the benefit of allowing user to choose between the different database dialects is marginal and the overhead of developing for multiple databases at once is significant.\\n\\n#### SQL <-> TypeScript\\n\\nAnother advantage of writing SQL is we can easily use it as the single source of truth of TypeScript definitions. I wrote a [code generator](https://github.com/logto-io/logto/tree/af7e6ccd83723d623555dafa4650e115fa795838/packages/schemas/src/gen) to transpile SQL schemas to TypeScript code that we\u2019ll use in our backend, and the result looks not bad:\\n\\n```tsx\\n// THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.\\n\\nimport { OidcClientMetadata } from \'../foundations\';\\n\\nexport type OidcClient = {\\n  clientId: string;\\n  metadata: OidcClientMetadata;\\n  createdAt: number;\\n};\\n// ...\\n```\\n\\nWe can even connect `jsonb` with a TypeScript type and process type validation in the backend service if needed.\\n\\n:::note \ud83e\udd14 Why not use TypeScript as the SSOT?\\nIt\u2019s a plan I\u2019ve thought of. It sounds attractive initially, but SQL will precisely describe database schemas and keep the flow in one direction (see the following section) instead of using TypeScript and then \u201ctranspile back\u201d to SQL.\\n:::\\n\\n### Result\\n\\nThe final dependency structure looks like:\\n\\n```mermaid\\ngraph TD\\n  database[Postgres Database] --\x3e Schemas\\n  subgraph Monorepo\\n\\t  Phrases --\x3e Core\\n\\t  Phrases --\x3e UI\\n\\t  Schemas --\x3e Core\\n\\t  Schemas --\x3e UI\\n  end\\n```\\n\\nYou may notice it\u2019s a one-direction diagram, which greatly helped us to keep a clear architecture and the ability to expand while the project grows. Plus, the code is (basically) all in TypeScript.\\n\\n## Dev experience\\n\\n### Package and config sharing\\n\\n#### Internal dependencies\\n\\n`pnpm` and `lerna` are doing an awesome job on internal workspace dependencies. We use the command below in the project root to add sibling packages:\\n\\n```bash\\nlerna add --scope=@logto/core @logto/schemas\\n```\\n\\nIt will add `@logto/schemas` as a dependency to `@logto/core`. While keeping the semantic version in `package.json` of your internal dependencies, `pnpm` can also correctly link them in `pnpm-lock.yaml`. The result will look like this:\\n\\n```json\\n// packages/core/pacakge.json\\n{\\n  \\"dependencies\\": {\\n    \\"@logto/schemas\\": \\"^1.0.0-beta.3\\"\\n  }\\n}\\n```\\n\\n```yaml\\n# pnpm-lock.yaml\\npackages/core:\\n  dependencies:\\n    \'@logto/schemas\': link:../schemas\\n```\\n\\n#### Config sharing\\n\\nWe treat every package in monorepo \u201cindependent\u201d. Thus we can use the standard approach for config sharing, which covers `tsconfig`, `eslintConfig`, `prettier`, `stlyelint`, and `jest-config`. See [this project](https://github.com/logto-io/logto/tree/6327eb6c577cdf36c8f44b55bac8195f7d6a6335/packages/console) for example.\\n\\n### Code, lint, and commit\\n\\nI use VSCode for daily development, and in short, nothing is different when the project is configured properly:\\n\\n- ESLint and Stylelint work normally.\\n    - If you are using VSCode ESLint plugin, add the VSCode settings below to make it honors the per-package ESLint config (replace the value of `pattern` with your own):\\n    \\n    ```json\\n    {\\n    \\t\\"eslint.workingDirectories\\": [\\n    \\t  {\\n    \\t    \\"pattern\\": \\"./packages/*\\"\\n    \\t  }\\n    \\t]\\n    }\\n    ```\\n    \\n- husky, commitlint, and lint-staged work as expected.\\n\\n### Compiler and proxy\\n\\nWe are using different compilers for frontend and backend: `parceljs` for UI (React) and `tsc` for all other pure TypeScript packages. I strongly recommend you to try `parceljs` if you haven\u2019t. It\u2019s a real \u201czero-config\u201d compiler that gracefully handles different file types.\\n\\nParcel hosts its own frontend dev server, and the production output is just static files. Since we\u2019d like to mount APIs and SPA under the same origin to avoid CORS issues, the strategy below works:\\n\\n- In dev environment, use a simple HTTP proxy to redirect the traffic to the Parcel dev server.\\n- In production, serve static files directly.\\n\\nYou can find the frontend middleware function implementation [here](https://github.com/logto-io/logto/blob/6327eb6c577cdf36c8f44b55bac8195f7d6a6335/packages/core/src/middleware/koa-spa-proxy.ts).\\n\\n### Watch mode\\n\\nWe have a `dev` script in `package.json` for each package that watches the file changes and re-compile when needed. Thanks to `lerna`, things become easy using `lerna exec` to run package scripts in parallel. The root script will look like this:\\n\\n```yaml\\n\\"dev\\": \\"lerna --scope=@logto/{core,phrases,schemas,ui} exec -- pnpm dev\\"\\n```\\n\\n### Summary\\n\\nIdeally, only two steps for a new engineer/contributor to get started:\\n\\n1. Clone the repo\\n2. `pnpm i && pnpm dev`\\n\\n## Closing notes\\n\\nOur team has been developing under this approach for one year, and we are pretty happy with it. Visit our [GitHub repo](https://github.com/logto-io/logto) to see the latest shape of the project. To wrap up:\\n\\n**Pains**\\n\\n- Need to be familiar with the JS/TS ecosystem\\n- Need to choose the right package manager\\n- Require some additional one-time setup\\n\\n**Gains**\\n\\n- Develop and maintain the whole project in one repo\\n- Simplified coding skill requirements\\n- Shared code styles, schemas, phrases, and utilities\\n- Improved communication efficiency\\n    - No more questions like: What\u2019s the API definition?\\n    - All engineers are talking in the same programming language\\n- CI/CD with ease\\n    - Use the same toolchain for building, testing, and publishing\\n\\nThis article remains several topics uncovered: Setting up the repo from scratch, adding a new package, leveraging GitHub Actions for CI/CD, etc. It\u2019ll be too long for this article if I expand each of them. Feel free to comment and let me know which topic you\u2019d like to see in the future."}]}')}}]);